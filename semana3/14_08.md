# Relat√≥rio de Estudos üìú

**Nome do Estagi√°rio:** Jo√£o Gabriel Barbosa <br>
**Data:** 14/08/2024

**M√≥dulos/Etapas Feitas:**  
1. **Linguagens e Frameworks**
2. **Mensageria**
3. **Virtualiza√ß√£o**
4. **Computa√ß√£o em Nuvem**

## Resumo dos m√≥dulos üìñ
Nessa trilha foi possivel analisar temas importantes para os estudos
como  **Linguagens e Frameworks, Mensageria, Virtualiza√ß√£o,    Computa√ß√£o em Nuvem** onde pudemos ver um pouco sobre Python apache spark e outras ferramentas, um pouco sobre virtualia√ß√£o e sua import√¢ncia o famoso **Docker** e muitos outros temas que vamos passar agora por t√≥picos:

## Linguagens e Frameworks üêç
Um dos pontos vistos foram **Python, Apache Spark, Apache Beame por fim Apache Airflow** de forma resumida pyhton uma das linguagens mais conhecida e famosas da modernidade pela sua facilidade e pela sua entrega e que roda em todo lugar at√© na torradeira, no caso vai rodar desktop, web e at√© mobile com algumas bibliotecas e frameworks, temos tamb√©m o Apache Spark um framework que tem como funcionamento o processamento de dados em uma grande quantidade oque vai nos oferecer um r√°pido desempenho e escalabilidade de projeto, Pyspark vai permitir usar o Spark junto com o Python √© ideal usar para a an√°lise de big data, machine lerning, J√° o Apache Beam √© um modelo open-source oque siginifica que √© de c√≥digo aberto para definir pipelines de dados em blocos  √© execut√°vel em diversos motores como o Google Dtaflow da Google e por fim vemos o Apche Airflow √© uma plataforma para a automatiza√ß√£o de workflows de dados que usar√° o Python para definir tarefas e depend√™ncias que vai ajudar no gerenciamento dos dados complexos 

## Mensageria üì®
√â uma t√©cnica de comunica√ß√£o entre sistemas distribu√≠dos que tem como fun√ß√£o prmitir a troca de mensagens de forma ass√≠ncrona e desacoplada. o Google cloud Pub/Sub √© um servi√ßo de menssageria que como nome ali em cima j√° diz como o pub/sub, oque vai oferecer uma ecalabilidade autom√°tica, baixa lat√™ncia e alta disponibilidade. Ele √© ideal para processamento de eventos em tempo real e integra√ß√£o de sistemas com benef√≠cios como escalabilidade de integra√ß√£o com outros servi√ßos do google cloud, desafios como gerenciamento de mensagens e custos.

## Virtualiza√ß√£o üñ•Ô∏è
Virtualiza√ß√£o o processo de gerar outra m√°quina ou containers dentro da sua propria m√°quina temos Hoje em dia a virtual box cujo a dona √© a a Oracle que tem como rpocesso gerar outras m√°quinas dentro d sua como se fosse um sandbox onde voce pode testar v√°rios sistemas operacionais ou softwares sua desvantagem √© que ela usa o hardware da sua m√°quina seu processador, mem√≥ria ram e por assim vai se seu computador n√£o for bom ser√° pess√≠mo pois n√£o ter√° um bom desempenho, Hoje gra√ßas as virtualiza√ß√µes nascessam os containers como o Docker que tem como fucnionalidade a conteineriza√ß√£o oque pode ser tipo como um ambien te virtual e com suas configura√ß√µes permite que voc√™ rode seu software em qualuqer computador j√° que a pessoa vai usar us√°ra o container para rodar e √© melhor doque vitualizar uma m√°quina com o virtual Box por ser mais leve e mais vers√°til tamb√©m vamos ter o kubernts que tem como papel orquestrar os containers automa√ß√£o e escalibilade oque vai ser ideal para microservi√ßos e para os ambientes do cloud mas como nem tudo e flores podemos encontrar as d√≠ficuldades pois e bem complexo e requer um conhecimento completo e complexo para sua utiliza√ß√£o

## Computa√ß√£o em Nuvem ‚òÅÔ∏è
Vamos falar um pouco do Google Cloud que √© um dos servi√ßos no mercado que nos oferece esse tipo de servi√ßo temos o DataFlow que tem como funcionaliade o processamento de dados em tempo real e batch com complexidade e com uma alta escalabilidade, √© baseado em Apache Beam. temos o Dataproc utiliza a Headoop e o Spark para fazer as grandes an√°lises e machine lerning, e termos os famosos clusters para as automatiza√ß√µes, Composer vai orquestar as workflows complexos usando o Apache Airflow de forma geral dentro de qualuqer servi√ßo em nuvem vai nos oferer ferramentas completas para a fornecer uma maior escalabilidade de dados fazendo com que seu porjeto ou produto tenha processamento maximo e de alta qualidade mas com grandes poderes vem grandes responsabilidades pois se as automa√ß√µes e processos maus otimizadados pode gerar custos altos e at√© absurdos que poderiam ser resduzidos com uma simples organiza√ß√£o de processo pois a computa√ß√£o em nuvem ela n√£o tem limites como as hospedagens mas sim possuem processamento infinito no caso usando da pol√≠tica o quanto mais voc√™ usar mais voc√™ paga por isso e nescess√°rio o entendimento dos processos em cada etapa para a melhor otimiza√ß√£o para o menor custo e melhor desempenho

## Links de Laborat√≥rios (se houver)
- [https://www.cloudskillsboost.google/paths/18/course_templates/578/labs/449087?locale=pt_BR](https://www.cloudskillsboost.google/paths/18/course_templates/578/labs/449087?locale=pt_BR)
- [https://www.cloudskillsboost.google/course_templates/623/labs/464851](https://www.cloudskillsboost.google/course_templates/623/labs/464851)
- [https://www.cloudskillsboost.google/course_templates/218/labs/496922](https://www.cloudskillsboost.google/course_templates/218/labs/496922)
- [Badge: Serverless Data Processing with Dataflow: Foundations](https://www.cloudskillsboost.google/public_profiles/a327db39-0839-45f9-8d75-350a5521e512/badges/10506979)


**Recursos Utilizados:**  
- **Documenta√ß√£o Linguagem**
- **Google**
- **Trilha de conhecimento**
- **Udemy**
- **Youtube**

## Principais comandos: (se aplic√°vel)

```python
df = spark.read.csv("caminho/do/arquivo.csv", header=True, inferSchema=True) # ler um arquivo csv
```
```python
df = spark.read.parquet("caminho/do/arquivo.parquet") # Ler um arquivo Parquet
```
```python
df = spark.read.json("caminho/do/arquivo.json") # Ler um arquivo JSON
```
```python
df.write.tipodoarquivo("caminho/do/saida.parquet") # Escrever um DataFrame
```
```python
df.show() # Mostrar as primeiras linhas do DataFrame
```
```python
df.select("coluna1", "coluna2").show() # Selecionar colunas
```
```python
df.filter(df["coluna"] > valor).show() # Filtro
```
```python
df.groupBy("coluna").count().show() # Agrupar por uma coluna e contar
```
```python
df2 = df.join(outro_df, df["coluna"] == outro_df["coluna"]) # Juntar DataFrames
```
```sql
SELECT * FROM nome_da_tabela; -- Selecionar dados de uma tabela
```
```sql
SELECT * FROM nome_da_tabela WHERE coluna2 > 10; -- Filtrar resultados
```
```sql
SELECT coluna1, COUNT(*) FROM nome_da_tabela GROUP BY coluna1; -- Agrupa os dados pela coluna 1 e conta quantos registros h√° em cada grupo
```
```sql
SELECT * FROM nome_da_tabela ORDER BY coluna2 DESC; -- Ordenar resultados
```
```sql
SELECT tabela_1.coluna1, tabela_2.coluna2
FROM tabela_1
JOIN tabela_2 ON tabela_1.coluna_pk = tabela_2.coluna_fk; -- INNER JOIN
```
```sql
SELECT tabela_1.coluna1 FROM tabela_1 FULL JOIN tabela_2 ON tabela_1.coluna_pk = tabela_2.coluna_fk -- FULL JOIN
```
```sql
SELECT tabela_1.coluna1 FROM tabela_1 CROSS JOIN tabela_2 -- CROSS JOIN
```

## Desafios Encontrados:
Nessa parte n√£o encontrei grandes dificuldades j√° que j√° possuia algumas familiariedades com alguns assuntos vistos na trilha.

## Feedback e Ajustes:
Deixo aqui meus sinceros feedback que a trilha e muito bom para aprendizado principalmente para o **Docker** que tivemos alguns exemplos e pude melhorar a minha l√≥gica de programa√ß√£o com python

## Pr√≥ximos Passos:
Pretendo no √∫ltimo reelat√≥rio terminar os estudos da trilha pra a proxima entrega
